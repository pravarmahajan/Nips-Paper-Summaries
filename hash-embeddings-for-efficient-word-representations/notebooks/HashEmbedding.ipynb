{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HashEmbedding(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_words, num_hash_functions, num_buckets, embedding_size, agg_function):\n",
    "        super(HashEmbedding, self).__init__()\n",
    "        self.num_words = num_words # K\n",
    "        self.num_hash_functions = num_hash_functions # k\n",
    "        self.num_buckets = num_buckets # B\n",
    "        self.embedding_size = embedding_size # d\n",
    "        self.W = nn.Parameter(torch.FloatTensor(num_buckets, embedding_size)) # B x d\n",
    "        self.agg_func = agg_function\n",
    "        self.hash_table = torch.LongTensor(np.random.randint(0, 2**30,\n",
    "                                size=(num_words, num_hash_functions)))%num_buckets # K x k\n",
    "        \n",
    "        self.P = nn.Parameter(torch.FloatTensor(num_words, num_hash_functions)) # K x k\n",
    "\n",
    "    \n",
    "    def forward(self, words_as_ids):\n",
    "        hashes = torch.index_select(self.hash_table, 0, words_as_ids)\n",
    "        z = torch.gather(self.hash_table, 0, hashes)\n",
    "        embeddings = []\n",
    "        for i in range(self.num_hash_functions):\n",
    "            embeddings.append(torch.mul(self.W[z[:, i]].t(), self.P[z[:, i]][:, i]).t())\n",
    "        cat_embeddings = torch.stack(embeddings, -1)\n",
    "        return self.agg_func(cat_embeddings, -1)\n",
    "    \n",
    "    def initializeWeights(self):\n",
    "        nn.init.normal(self.W, 0, 0.1)\n",
    "        nn.init.normal(self.P, 0, 0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 10**6\n",
    "num_hash = 2\n",
    "num_buckets = 5000\n",
    "embedding_dim = 20\n",
    "agg_function = torch.sum\n",
    "\n",
    "embedding_model = HashEmbedding(max_words, num_hash, num_buckets, embedding_dim, agg_function)\n",
    "embedding_model.initializeWeights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " -6.5621e-02 -2.9306e-01  7.4280e-02  ...  -8.2494e-02 -9.6310e-02 -2.9069e-02\n",
       "  2.2511e-02 -2.3906e-02 -9.9755e-02  ...   1.6616e-01  1.3403e-03 -6.1546e-02\n",
       " -1.4984e-01  2.5936e-02  2.6952e-02  ...  -8.3221e-02 -8.0320e-02 -6.5011e-02\n",
       "                 ...                   ⋱                   ...                \n",
       " -2.0936e-01  1.9901e-01 -1.0038e-01  ...   9.7263e-02  2.1844e-02  1.7956e-01\n",
       " -5.3256e-02  3.2948e-02  3.0606e-02  ...  -4.7108e-02 -2.0300e-02 -5.2797e-02\n",
       " -8.7721e-02  1.6017e-01 -8.0196e-02  ...   1.2384e-02  8.0157e-02  6.2865e-03\n",
       " [torch.FloatTensor of size 5000x20], Parameter containing:\n",
       "  8.0226e-04  5.3378e-04\n",
       " -3.3916e-04  1.8015e-04\n",
       " -3.3546e-04 -8.7273e-04\n",
       "            ⋮            \n",
       "  7.3516e-04 -4.6255e-04\n",
       " -3.8472e-04 -1.8639e-04\n",
       " -1.1415e-03 -4.2809e-04\n",
       " [torch.FloatTensor of size 1000000x2]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(embedding_model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HashEmbedding(\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
